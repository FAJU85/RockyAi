server:
  host: 0.0.0.0
  port: 11434

models:
  - name: ai/llama3
    when:
      gpu: true
      min_vram_gb: 10
  - name: ai/llama2
    when:
      gpu: false

routing:
  default: ai/llama2
  gpu_preferred: ai/llama3

# Model-specific configurations
model_configs:
  ai/llama2:
    context_length: 4096
    temperature: 0.7
    top_p: 0.9
    max_tokens: 2048
    
  ai/llama3:
    context_length: 8192
    temperature: 0.7
    top_p: 0.9
    max_tokens: 4096

# Resource management
resources:
  max_concurrent_requests: 4
  request_timeout: 300
  memory_limit_gb: 8